{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nYGdw9mTubp"
   },
   "source": [
    "# LSH Spark tutorial\n",
    "In questo tutorial mostreremo come usare le funzioni della libreria MLLib di Spark per implementare l'LSH per vettori sparsi per il calcolo della similarià di Jaccard e per vettori denzi per la distanza Euclidea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ky4eX8mmzyJW"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.mllib import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mye4whJR6vlU"
   },
   "source": [
    "Creiamo la sessione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qB0fiLchz5LW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/29 18:15:52 WARN Utils: Your hostname, TeoAcerAspireOne resolves to a loopback address: 127.0.1.1; using 192.168.1.88 instead (on interface wlp0s20f3)\n",
      "25/03/29 18:15:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/29 18:15:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# create the session\n",
    "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
    "\n",
    "# create the context\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObMpx-BB6xra"
   },
   "source": [
    "Importiamo le librerie necessarie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "t5_kk0AmOchH"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinHashLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqYE6KcKQUv5"
   },
   "source": [
    "# Distanza di Jaccard su vettori sparsi\n",
    "\n",
    "Creiamo un dataframe con 3 vettori sparis. Un vettore sparso in spark è composto da tre elementi. La dimensione, un vettore di indici dove il vettore assume valori diversi da 0 e un vettore contenente i valori effettivi del dato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2Cj3EPh2OitN"
   },
   "outputs": [],
   "source": [
    "dataA = [(0, Vectors.sparse(6, [0, 1, 2], [1.0, 1.0, 1.0]),),\n",
    "         (1, Vectors.sparse(6, [2, 3, 4], [1.0, 1.0, 1.0]),),\n",
    "         (2, Vectors.sparse(6, [0, 2, 4], [1.0, 1.0, 1.0]),)]\n",
    "dfA = spark.createDataFrame(dataA, [\"id\", \"features\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3f493d-kR51Z"
   },
   "source": [
    "Creiamo un secondo dataframe di 3 vettori sparsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_ZBzhZe9QxQy"
   },
   "outputs": [],
   "source": [
    "dataB = [(3, Vectors.sparse(6, [1, 3, 5], [1.0, 1.0, 1.0]),),\n",
    "         (4, Vectors.sparse(6, [2, 3, 5], [1.0, 1.0, 1.0]),),\n",
    "         (5, Vectors.sparse(6, [1, 2, 4], [1.0, 1.0, 1.0]),)]\n",
    "dfB = spark.createDataFrame(dataB, [\"id\", \"features\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eO2kCV43Q7A0"
   },
   "source": [
    "Creiamo un vettore sparso per effettuare un knn query su uno dei due dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3z8zLVUQQ09r"
   },
   "outputs": [],
   "source": [
    "key = Vectors.sparse(6, [1, 3], [1.0, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCRmdrx8RHWn"
   },
   "source": [
    "Creiamo il modello con la funzione MinHashLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6cF-8uVuRLPi",
    "outputId": "e8460a36-458a-40b8-d3f8-e3a2edce4b3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il set di dati con hash, in cui i valori con hash sono memorizzati nella colonna 'hash'.:\n",
      "+---+-------------------------+------------------------------------------------------------------------------+\n",
      "|id |features                 |hashes                                                                        |\n",
      "+---+-------------------------+------------------------------------------------------------------------------+\n",
      "|0  |(6,[0,1,2],[1.0,1.0,1.0])|[[4.0993917E7], [3.1447487E8], [4.9332997E8], [1.56257325E8], [4.69971411E8]] |\n",
      "|1  |(6,[2,3,4],[1.0,1.0,1.0])|[[4.0993917E7], [1.54244661E8], [5.47929187E8], [1.56257325E8], [7.4248372E7]]|\n",
      "|2  |(6,[0,2,4],[1.0,1.0,1.0])|[[4.0993917E7], [1.54244661E8], [4.9332997E8], [1.56257325E8], [7.4248372E7]] |\n",
      "+---+-------------------------+------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=5)\n",
    "model = mh.fit(dfA)\n",
    "\n",
    "# Feature Transformation\n",
    "print(\"Il set di dati con hash, in cui i valori con hash sono memorizzati nella colonna 'hash'.:\")\n",
    "model.transform(dfA).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEm0pr6yR_gA"
   },
   "source": [
    "Confrontiamo i due dataframe dfA e dfB per ricercare coppie di vettori con una distanza minore o uguale a 0.6. Si potrebbe direttamente effettuare il calcolo della distanza tra le coppie di già trasformate\n",
    "\n",
    "```\n",
    "model.approxSimilarityJoin(transformedA, transformedB, 0.6)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-gZqNXzxrrUw",
    "outputId": "2de9d7b9-5b40-46e8-c242-68db3f4a6a82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cerchiamo le coppie di punti nei due dataframe con una distanza minore di 0.6:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------------+\n",
      "|idA|idB|JaccardDistance|\n",
      "+---+---+---------------+\n",
      "|  1|  4|            0.5|\n",
      "|  1|  5|            0.5|\n",
      "|  0|  5|            0.5|\n",
      "|  2|  5|            0.5|\n",
      "+---+---+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Cerchiamo le coppie di punti nei due dataframe con una distanza minore di 0.6:\")\n",
    "model.approxSimilarityJoin(dfA, dfB, 0.6, distCol=\"JaccardDistance\")\\\n",
    "    .select(col(\"datasetA.id\").alias(\"idA\"),\n",
    "            col(\"datasetB.id\").alias(\"idB\"),\n",
    "            col(\"JaccardDistance\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fp7Y14qC7D48"
   },
   "source": [
    "Risolviamo il problema del k-nearest neighbour. Anche in questo caso potremmo usare direttamente la fuzione\n",
    "\n",
    "\n",
    "```\n",
    "model.approxNearestNeighbors(transformedA, key, 2)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MaZPY0wZ7JzZ",
    "outputId": "28eb945f-fd0a-4ccb-a7e4-6f0a9e64fa50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cercare approssimativamente in dfA i 2 vicini più prossimi della chiave:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+-------+\n",
      "| id|features|hashes|distCol|\n",
      "+---+--------+------+-------+\n",
      "+---+--------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Può restituire un numero minore di 2 righe quando non sono identificati un numero sufficiente di candidati\n",
    "print(\"Cercare approssimativamente in dfA i 2 vicini più prossimi della chiave:\")\n",
    "model.approxNearestNeighbors(dfA, key, 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3yVyWJ97nEK"
   },
   "source": [
    "# Distanza Euclidea e vettori densi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "kbvukgDO0eEw"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import BucketedRandomProjectionLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dl8sKoN-TiVv",
    "outputId": "cbc9ad8a-e74a-427c-9ee0-ee21b4d6823c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/29 19:39:09 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il set di dati con  il valore hash:\n",
      "+---+-----------+--------------------+\n",
      "| id|   features|              hashes|\n",
      "+---+-----------+--------------------+\n",
      "|  0|  [1.0,1.0]|[[-1.0], [-1.0], ...|\n",
      "|  1| [1.0,-1.0]|[[-1.0], [0.0], [...|\n",
      "|  2|[-1.0,-1.0]|[[0.0], [0.0], [0...|\n",
      "|  3| [-1.0,1.0]|[[0.0], [-1.0], [...|\n",
      "+---+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataA = [(0, Vectors.dense([1.0, 1.0]),),\n",
    "         (1, Vectors.dense([1.0, -1.0]),),\n",
    "         (2, Vectors.dense([-1.0, -1.0]),),\n",
    "         (3, Vectors.dense([-1.0, 1.0]),)]\n",
    "dfA = spark.createDataFrame(dataA, [\"id\", \"features\"])\n",
    "\n",
    "dataB = [(4, Vectors.dense([1.0, 0.0]),),\n",
    "         (5, Vectors.dense([-1.0, 0.0]),),\n",
    "         (6, Vectors.dense([0.0, 1.0]),),\n",
    "         (7, Vectors.dense([0.0, -1.0]),)]\n",
    "dfB = spark.createDataFrame(dataB, [\"id\", \"features\"])\n",
    "\n",
    "key = Vectors.dense([1.0, 0.0])\n",
    "\n",
    "brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\", bucketLength=2.0,\n",
    "                                  numHashTables=3)\n",
    "model = brp.fit(dfA)\n",
    "\n",
    "# Feature hashing\n",
    "print(\"Il set di dati con  il valore hash:\")\n",
    "model.transform(dfA).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07N6YkzbzEOy"
   },
   "source": [
    "Calcoliamo l'LSH delle righe di input ed effettuiamo una ricerca approssimata.\n",
    "Potremmo applicarlo direttamente al dato mappato con l'hash\n",
    "```\n",
    "model.approxSimilarityJoin(transformedA, transformedB, 1.5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hUYinTrMzEmu",
    "outputId": "f702c7c4-7200-4358-9962-303a5cd2bf9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join approssimata tra dfA e dfB con una distanza Euclidea minore di 1.5:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------------+\n",
      "|idA|idB|EuclideanDistance|\n",
      "+---+---+-----------------+\n",
      "|  0|  6|              1.0|\n",
      "|  2|  7|              1.0|\n",
      "|  0|  4|              1.0|\n",
      "|  2|  5|              1.0|\n",
      "|  1|  7|              1.0|\n",
      "|  3|  5|              1.0|\n",
      "|  1|  4|              1.0|\n",
      "|  3|  6|              1.0|\n",
      "+---+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Join approssimata tra dfA e dfB con una distanza Euclidea minore di 1.5:\")\n",
    "model.approxSimilarityJoin(dfA, dfB, 1.5, distCol=\"EuclideanDistance\")\\\n",
    "    .select(col(\"datasetA.id\").alias(\"idA\"),\n",
    "            col(\"datasetB.id\").alias(\"idB\"),\n",
    "            col(\"EuclideanDistance\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7QP_RRazq2t"
   },
   "source": [
    "Calcolo della nearest neighbor approssimata\n",
    "anche in questo caso possiamo usare i dati gia' trasformati\n",
    "```\n",
    "model.approxNearestNeighbors(transformedA, key, 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YeAsZDZFzrOa",
    "outputId": "03d027f2-46b7-4744-bf56-561ef21397b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ricerca approssimata nel dfA per i due 2 nearest neighbor della chiave:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+-------+\n",
      "| id|  features|              hashes|distCol|\n",
      "+---+----------+--------------------+-------+\n",
      "|  0| [1.0,1.0]|[[-1.0], [-1.0], ...|    1.0|\n",
      "|  1|[1.0,-1.0]|[[-1.0], [0.0], [...|    1.0|\n",
      "+---+----------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Ricerca approssimata nel dfA per i due 2 nearest neighbor della chiave:\")\n",
    "model.approxNearestNeighbors(dfA, key, 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MsNNHOH7ujh"
   },
   "source": [
    "Comprendre al meglio l'uso della libreria.\n",
    "\n",
    "\n",
    "# Esercizio (consegna il 7 aprile 2025)\n",
    "\n",
    "\n",
    "1.   Scaricare i dati disponibili a questo url: [amazon reviews](https://www.kaggle.com/datasets/kritanjalijain/amazon-reviews?resource=download) o questo [amazon review dropbox](https://www.dropbox.com/scl/fi/ucfoh391qalha3lz0bzjx/amazon_review_polarity_csv.tgz.zip?rlkey=m3a0bbp2ep4sh2qisaz0xwo1w&dl=0)\n",
    "\n",
    "\n",
    "2.   Il dataset è composto da due file: train and test in csv. Ogni file contiene le seguenti informazioni\n",
    "  *   polarity - 1 for negative and 2 for positive\n",
    "  *   title - review heading\n",
    "  *   text - review body\n",
    "\n",
    "3.  Generare i vettori sparsi applicando il q-shingle ai dati di training con q=3.\n",
    "4. Sui vettori sparsi Applicare il MinHashing LSH sul dataset di training.\n",
    "5. USare il file di testing e applicare una k-nearest neighbor con i dati di testing su cui è stato applicato l'hashing. Usare k=3 e classificare l'elemento con del test set con la polarità maggiormente presente.\n",
    "\n",
    "\n",
    "6. *Identificare i cluster di recensioni. Ogni cluster di recensione contiene le coppie di recensioni che hanno una similarità  > di 0.6. Da svolgere dopo l'introduzione alle network\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "8QvOa8IJsDjC"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "\n",
    "The files train.csv and test.csv contain all the training samples as comma-separated values.\n",
    "\n",
    "The CSVs contain polarity, title, text. These 3 columns in them, correspond to class index (1 or 2), review title and review text.\n",
    "\n",
    "    polarity - 1 for negative and 2 for positive\n",
    "    title - review heading\n",
    "    text - review body\n",
    "\n",
    "The review title and text are escaped using double quotes (\"), and any internal double quote is escaped by 2 double quotes (\"\"). New lines are escaped by a backslash followed with an \"n\" character, that is \"\\n\".\n",
    "'''\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"polarity\", IntegerType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"text\", StringType(), True)])\n",
    "\n",
    "spark = SparkSession.builder.appName(\"AmazonReviews\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- polarity: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:==============================================>         (10 + 2) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data rows: 3600000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "    train = spark.read.csv(\"/home/teogalletta/Downloads/amazon_review_polarity_csv/train.csv\", schema=schema, escape='\"', quote='\"')\n",
    "train.printSchema()\n",
    "print(\"Train data rows:\", train.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- polarity: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data rows: 400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test = spark.read.csv(\"/home/teogalletta/Downloads/amazon_review_polarity_csv/test.csv\", schema=schema, escape='\"', quote='\"')\n",
    "test.printSchema()\n",
    "print(\"Test data rows:\", test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingle_udf(text, n=3):\n",
    "    \"\"\"Genera n-gramas (shingles) a partir de una cadena de texto.\"\"\"\n",
    "    chars = list(text)\n",
    "    return [\"\".join(chars[i:i+n]) for i in range(len(chars) - n + 1)] if len(chars) >= n else []\n",
    "\n",
    "shingle = udf(lambda text: shingle_udf(text, 3), ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w_shingles = train.withColumn(\"shingles\", shingle(col(\"text\")))\n",
    "test_w_shingles = test.withColumn(\"shingles\", shingle(col(\"text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+\n",
      "|polarity|               title|                text|            shingles|\n",
      "+--------+--------------------+--------------------+--------------------+\n",
      "|       2|Stuning even for ...|This sound track ...|[Thi, his, is , s...|\n",
      "+--------+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_w_shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column shingles must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<string>.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIllegalArgumentException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m mh = MinHashLSH(inputCol=\u001b[33m\"\u001b[39m\u001b[33mshingles\u001b[39m\u001b[33m\"\u001b[39m, outputCol=\u001b[33m\"\u001b[39m\u001b[33mhashes\u001b[39m\u001b[33m\"\u001b[39m, numHashTables=\u001b[32m5\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model = \u001b[43mmh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_w_shingles\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/my-obsidian-vault/100 university/3202 Big Data/exercises/.venv/lib/python3.13/site-packages/pyspark/ml/base.py:205\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._fit(dataset)\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    208\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    209\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[32m    210\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/my-obsidian-vault/100 university/3202 Big Data/exercises/.venv/lib/python3.13/site-packages/pyspark/ml/wrapper.py:381\u001b[39m, in \u001b[36mJavaEstimator._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) -> JM:\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m     java_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    382\u001b[39m     model = \u001b[38;5;28mself\u001b[39m._create_model(java_model)\n\u001b[32m    383\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._copyValues(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/my-obsidian-vault/100 university/3202 Big Data/exercises/.venv/lib/python3.13/site-packages/pyspark/ml/wrapper.py:378\u001b[39m, in \u001b[36mJavaEstimator._fit_java\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28mself\u001b[39m._transfer_params_to_java()\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_java_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/my-obsidian-vault/100 university/3202 Big Data/exercises/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/my-obsidian-vault/100 university/3202 Big Data/exercises/.venv/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mIllegalArgumentException\u001b[39m: requirement failed: Column shingles must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<string>."
     ]
    }
   ],
   "source": [
    "mh = MinHashLSH(inputCol=\"shingles\", outputCol=\"hashes\", numHashTables=5)\n",
    "model = mh.fit(train_w_shingles)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
